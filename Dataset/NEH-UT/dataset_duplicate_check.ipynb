{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d7ba4f-ce54-4a4d-900d-25d71567b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 16,803 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|████████████████████████████████████████████████████████| 16803/16803 [03:16<00:00, 85.68img/s]\n",
      "Hashing (pHash-16): 100%|██████████████████████████████████████████████████████| 16803/16803 [00:51<00:00, 324.55img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "\n",
      "[1] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\000_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\000_Normal.tif\n",
      "\n",
      "[2] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\001_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\001_Normal.tif\n",
      "\n",
      "[3] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\002_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\002_Normal.tif\n",
      "\n",
      "[4] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\003_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\003_Normal.tif\n",
      "\n",
      "[5] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\004_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\004_Normal.tif\n",
      "\n",
      "[6] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\005_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\005_Drusen.tif\n",
      "\n",
      "[7] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\006_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\006_Drusen.tif\n",
      "\n",
      "[8] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\007_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\007_Drusen.tif\n",
      "\n",
      "[9] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\008_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\008_Drusen.tif\n",
      "\n",
      "[10] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\009_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\009_Drusen.tif\n",
      "\n",
      "[11] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\010_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\010_Drusen.tif\n",
      "\n",
      "[12] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\011_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\011_Drusen.tif\n",
      "\n",
      "[13] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\012_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\012_Drusen.tif\n",
      "\n",
      "[14] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\013_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\013_Drusen.tif\n",
      "\n",
      "[15] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\014_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\014_Drusen.tif\n",
      "\n",
      "[16] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\015_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\015_Drusen.tif\n",
      "\n",
      "[17] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\016_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\016_Drusen.tif\n",
      "\n",
      "[18] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\017_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\017_Drusen.tif\n",
      "\n",
      "[19] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\018_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\018_Drusen.tif\n",
      "\n",
      "[20] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\019_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\019_Drusen.tif\n",
      "\n",
      "[21] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\020_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\020_Drusen.tif\n",
      "\n",
      "[22] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\021_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\021_Drusen.tif\n",
      "\n",
      "[23] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\022_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\022_Drusen.tif\n",
      "\n",
      "[24] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\023_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\023_Drusen.tif\n",
      "\n",
      "[25] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\024_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\024_Drusen.tif\n",
      "\n",
      "[26] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\025_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\025_Drusen.tif\n",
      "\n",
      "[27] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\026_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\026_Drusen.tif\n",
      "\n",
      "[28] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\027_Drusen.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\027_Drusen.tif\n",
      "\n",
      "[29] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\028_Drusen.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\028_Drusen.tif\n",
      "\n",
      "[30] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\029_Drusen.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\029_Drusen.tif\n",
      "\n",
      "... (showing first 30 groups, 88 more not shown)\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "\n",
      "[1] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\000_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\000_Normal.tif\n",
      "\n",
      "[2] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\001_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\001_Normal.tif\n",
      "\n",
      "[3] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\002_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\002_Normal.tif\n",
      "\n",
      "[4] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\003_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\003_Normal.tif\n",
      "\n",
      "[5] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\004_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\004_Normal.tif\n",
      "\n",
      "[6] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\005_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\005_Drusen.tif\n",
      "\n",
      "[7] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\006_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\006_Drusen.tif\n",
      "\n",
      "[8] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\007_Normal.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\007_Drusen.tif\n",
      "\n",
      "[9] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\008_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\008_Drusen.tif\n",
      "\n",
      "[10] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\009_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\009_Drusen.tif\n",
      "\n",
      "[11] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\010_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\010_Drusen.tif\n",
      "\n",
      "[12] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\011_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\011_Drusen.tif\n",
      "\n",
      "[13] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\012_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\012_Drusen.tif\n",
      "\n",
      "[14] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\013_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\013_Drusen.tif\n",
      "\n",
      "[15] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\014_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\014_Drusen.tif\n",
      "\n",
      "[16] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\015_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\015_Drusen.tif\n",
      "\n",
      "[17] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\016_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\016_Drusen.tif\n",
      "\n",
      "[18] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\017_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\017_Drusen.tif\n",
      "\n",
      "[19] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\018_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\018_Drusen.tif\n",
      "\n",
      "[20] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\019_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\019_Drusen.tif\n",
      "\n",
      "[21] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\020_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\020_Drusen.tif\n",
      "\n",
      "[22] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\021_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\021_Drusen.tif\n",
      "\n",
      "[23] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\022_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\022_Drusen.tif\n",
      "\n",
      "[24] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\023_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\023_Drusen.tif\n",
      "\n",
      "[25] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\024_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\024_Drusen.tif\n",
      "\n",
      "[26] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\025_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\025_Drusen.tif\n",
      "\n",
      "[27] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\026_CNV.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\026_Drusen.tif\n",
      "\n",
      "[28] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\027_Drusen.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\027_Drusen.tif\n",
      "\n",
      "[29] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\028_Drusen.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\028_Drusen.tif\n",
      "\n",
      "[30] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\CNV\\22\\029_Drusen.tif\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\DRUSEN\\69\\029_Drusen.tif\n",
      "\n",
      "... (showing first 30 groups, 88 more not shown)\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 16,803\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 118 (cross-split groups: 0)\n",
      "Visual duplicate groups: 118 (cross-split groups: 0)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\\dup_reports\\duplicates_combined.csv\n",
      "      group_type                                          group_key  \\\n",
      "0   exact_sha256  548c7a5caf92d8f9de346b3269b8c3a8357434b99c7fbd...   \n",
      "1   exact_sha256  33640de0876b102ea9cdf3e11ccf5e94112d13ad5e9b23...   \n",
      "2   exact_sha256  64bc3a9c83fe984dcf0bff16f5fd93e81ea7958670cf3e...   \n",
      "3   exact_sha256  af7967f70a299881231b952c6262cfd3dc9a8ea5e676e2...   \n",
      "4   exact_sha256  51c71eaad25b492267bdaeaacfbe5e643bda521c3cc69d...   \n",
      "5   exact_sha256  2f6a5c8d03c53588444d1c31ef27eeaeea09421b021c2d...   \n",
      "6   exact_sha256  fd0b7b1c25950e366b440a640d3257220fc0605cf2986c...   \n",
      "7   exact_sha256  38dba48a340f93e3a3bcbcf9a8c8fad9dec4589521271d...   \n",
      "8   exact_sha256  274a470aff6f0ee91096f2f3d43c1f5d269042ae674bb8...   \n",
      "9   exact_sha256  b8fca440d8da7847ceb543d36d693533ee89cee81ad156...   \n",
      "10  exact_sha256  6c0a5705c8cfb6065e3df3496f430632f9d204263c9fa1...   \n",
      "11  exact_sha256  5a037d020028137d7c40fcaee1ae47ed37fe519ef8aebc...   \n",
      "12  exact_sha256  db919a492178c2f3956b8a0d0290283eb455447e4cff45...   \n",
      "13  exact_sha256  f9c3af95cd86d286d092fc6d0be068c572de3fd55d433f...   \n",
      "14  exact_sha256  cbb1de9de9076a7ac2ffcc6a067c54297459a065e29827...   \n",
      "15  exact_sha256  0694c08d33619ae2031222f1b96be516b55936c392166b...   \n",
      "16  exact_sha256  fa480a5068f42957ecbebc014a2ecccef294e19a8a3de3...   \n",
      "17  exact_sha256  35600e7b07a02ffae5d93fa7d1caf8cbb2fcc2e8302ed8...   \n",
      "18  exact_sha256  d24c81b854ca84e7f0918aef0b68770f99e68012336472...   \n",
      "19  exact_sha256  f306ed6bd43c978ad56bf1103a86d171f253b16f78f92b...   \n",
      "\n",
      "                                       canonical_path  duplicate_count  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...                1   \n",
      "\n",
      "                                      duplicate_paths  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...   \n",
      "\n",
      "                                            all_paths splits_involved  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOC...         unknown   \n",
      "\n",
      "    cross_split  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "5             0  \n",
      "6             0  \n",
      "7             0  \n",
      "8             0  \n",
      "9             0  \n",
      "10            0  \n",
      "11            0  \n",
      "12            0  \n",
      "13            0  \n",
      "14            0  \n",
      "15            0  \n",
      "16            0  \n",
      "17            0  \n",
      "18            0  \n",
      "19            0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1832f80a-1bc7-4d8b-9558-fae8e95cc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 16,663 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2_CLEAN_SHAONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|███████████████████████████████████████████████████████| 16663/16663 [00:43<00:00, 383.22img/s]\n",
      "Hashing (pHash-16): 100%|██████████████████████████████████████████████████████| 16663/16663 [01:00<00:00, 273.87img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 16,663\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 0 (cross-split groups: 0)\n",
      "Visual duplicate groups: 0 (cross-split groups: 0)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2_CLEAN_SHAONLY\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2_CLEAN_SHAONLY\\dup_reports\\duplicates_combined.csv\n",
      "Empty DataFrame\n",
      "Columns: [group_type, group_key, canonical_path, duplicate_count, duplicate_paths, all_paths, splits_involved, cross_split]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\NEH_UT_2021RetinalOCTDataset_V2_CLEAN_SHAONLY\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80320b99-ce6e-4a35-bfef-e9197f2971a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
