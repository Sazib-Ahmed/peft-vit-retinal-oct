{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90889c3-ca6e-4d9a-8612-d1933289e6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ROOT_DIR: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\n",
      "[INFO] CSV_PATH: None\n",
      "[INFO] MODE: auto | detected_has_splits: True\n",
      "\n",
      "==========================================================================================\n",
      "[STATS] BEFORE DEDUPLICATION (raw manifest)\n",
      "==========================================================================================\n",
      "Total images: 24,000\n",
      "\n",
      "Images per split:\n",
      "split\n",
      "train    18400\n",
      "test      2800\n",
      "val       2800\n",
      "\n",
      "Images per class:\n",
      "label\n",
      "AMD       3000\n",
      "CNV       3000\n",
      "CSR       3000\n",
      "DME       3000\n",
      "DR        3000\n",
      "DRUSEN    3000\n",
      "MH        3000\n",
      "NORMAL    3000\n",
      "\n",
      "Images per split x class:\n",
      "label   AMD   CNV   CSR   DME    DR  DRUSEN    MH  NORMAL\n",
      "split                                                    \n",
      "test    350   350   350   350   350     350   350     350\n",
      "train  2300  2300  2300  2300  2300    2300  2300    2300\n",
      "val     350   350   350   350   350     350   350     350\n",
      "\n",
      "Unique patients (patient_uid): N/A (not provided)\n",
      "[INFO] Computing SHA-256 (exact duplicates only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sha256: 100%|██████████████████████████████████████████████████████████████████| 24000/24000 [01:49<00:00, 219.84img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] After SHA-256: kept 23,850 images\n",
      "[INFO] SHA-256 dedupe stats: {'bad_hash_reads': 0, 'duplicate_groups_sha256': 144, 'removed_duplicates_sha256': 138, 'removed_conflicts_sha256': 12, 'kept_after_sha256': 23850}\n",
      "\n",
      "==========================================================================================\n",
      "[STATS] AFTER SHA-256 DEDUPLICATION (and split assignment)\n",
      "==========================================================================================\n",
      "Total images: 23,850\n",
      "\n",
      "Images per split:\n",
      "split\n",
      "train    18260\n",
      "test      2797\n",
      "val       2793\n",
      "\n",
      "Images per class:\n",
      "label\n",
      "AMD       3000\n",
      "CSR       3000\n",
      "MH        3000\n",
      "DR        3000\n",
      "NORMAL    2996\n",
      "DME       2970\n",
      "CNV       2954\n",
      "DRUSEN    2930\n",
      "\n",
      "Images per split x class:\n",
      "label   AMD   CNV   CSR   DME    DR  DRUSEN    MH  NORMAL\n",
      "split                                                    \n",
      "test    350   350   350   349   350     349   350     349\n",
      "train  2300  2257  2300  2272  2300    2233  2300    2298\n",
      "val     350   347   350   349   350     348   350     349\n",
      "\n",
      "Unique patients (patient_uid): N/A (not provided)\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY] KEY NUMBERS\n",
      "==========================================================================================\n",
      "Before: 24,000 images\n",
      "After : 23,850 images\n",
      "Removed exact duplicates (kept one per SHA group): 138\n",
      "Removed label-conflict groups (dropped all in group): 12\n",
      "Bad/unreadable during hashing: 0\n",
      "Duplicate groups found (SHA): 144\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Writing output dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing: 100%|████████████████████████████████████████████████████████████████| 23850/23850 [00:10<00:00, 2384.96img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Images per split/label:\n",
      " label   AMD   CNV   CSR   DME    DR  DRUSEN    MH  NORMAL\n",
      "split                                                    \n",
      "test    350   350   350   349   350     349   350     349\n",
      "train  2300  2257  2300  2272  2300    2233  2300    2298\n",
      "val     350   347   350   349   350     348   350     349\n",
      "\n",
      "[DONE] Output root: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset_CLEAN_SHAONLY\n",
      "[DONE] Reports: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset_CLEAN_SHAONLY\\reports\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "UNIVERSAL OCT DATASET CLEANER (SHA-256 ONLY)\n",
    "\n",
    "What it does (NO pHash / visual dedupe):\n",
    "1) If dataset has NO train/val/test split and has a CSV with patient_id (e.g., NEH/OCTDL):\n",
    "   - Build manifest from CSV (or scan if CSV not provided)\n",
    "   - SHA-256 exact dedupe (drop label-conflicts)\n",
    "   - Patient-wise stratified split 70/15/15 (no leakage) if patient IDs exist\n",
    "   - Materialize OUT/train|val|test/<label>/\n",
    "2) If dataset ALREADY has train/val/test split (e.g., C8):\n",
    "   - Scan folders root/{train,val,test}/{label}/...\n",
    "   - SHA-256 exact dedupe with split priority (KEEP: test > val > train)\n",
    "   - Materialize OUT with same split structure (does NOT reshuffle)\n",
    "3) If dataset has NO CSV:\n",
    "   - If already split: scan folders\n",
    "   - If not split: scan ROOT/<label>/... and do IMAGE-wise split (warns: no patient leakage protection)\n",
    "\n",
    "Extra:\n",
    "- Prints detailed dataset stats BEFORE and AFTER:\n",
    "  * total images\n",
    "  * images per split\n",
    "  * images per class\n",
    "  * images per split x class\n",
    "  * #unique patients (if available)\n",
    "  * #images removed as duplicates, #removed as label-conflicts, #bad/unreadable\n",
    "- Writes reports to: OUT_DIR/reports/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import hashlib\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install deps (optional)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"numpy\", \"numpy\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "        (\"sklearn\", \"scikit-learn\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT THESE)\n",
    "# ============================================================\n",
    "\n",
    "# Dataset root (input)\n",
    "ROOT_DIR = Path(r\"D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\")  # change per dataset\n",
    "\n",
    "# Optional CSV (set to None if dataset has no CSV)\n",
    "CSV_PATH = None\n",
    "# Examples:\n",
    "# CSV_PATH = Path(r\"D:\\AIUB\\DSP\\Code\\Datasets\\OCTDL\\OCTDL_labels.csv\")\n",
    "# CSV_PATH = Path(r\"D:\\AIUB\\DSP\\Code\\Datasets\\NEH_UT_2021RetinalOCTDataset\\V2\\data_information.csv\")\n",
    "\n",
    "# Output root (will be created)\n",
    "OUT_DIR = Path(r\"D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset_CLEAN_SHAONLY\")\n",
    "\n",
    "# Mode: \"auto\" (recommended), \"preserve_splits\", \"build_splits\"\n",
    "MODE = \"auto\"\n",
    "\n",
    "# Split ratios (only used when building splits)\n",
    "RANDOM_SEED = 42\n",
    "SPLIT_RATIOS = {\"train\": 0.70, \"val\": 0.15, \"test\": 0.15}\n",
    "\n",
    "# Split folder names we recognize\n",
    "SPLITS = {\"train\", \"val\", \"valid\", \"validation\", \"test\"}\n",
    "SPLIT_CANON = {\"valid\": \"val\", \"validation\": \"val\"}\n",
    "\n",
    "# If duplicates span splits, keep higher-priority split:\n",
    "# test > val > train\n",
    "SPLIT_PRIORITY = {\"test\": 0, \"val\": 1, \"train\": 2, \"unknown\": 3}\n",
    "\n",
    "# File extensions\n",
    "VALID_EXT = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "# Output method\n",
    "COPY_MODE = \"hardlink\"  # \"copy\" | \"hardlink\" | \"symlink\"\n",
    "\n",
    "# If you have patient IDs and want extra conservative exact dedupe:\n",
    "# True => if same sha appears across different patients (same label), drop all those copies\n",
    "DROP_CROSS_PATIENT_EXACT_DUPES = False\n",
    "\n",
    "# CSV profile: \"auto\", \"NEH\", \"OCTDL\", \"GENERIC\"\n",
    "CSV_PROFILE = \"auto\"\n",
    "\n",
    "# NEH-specific (only used when CSV_PROFILE resolves to NEH)\n",
    "# FILTER_MODE: \"all\" or \"worstcase\" (keep only rows where Class == Label)\n",
    "FILTER_MODE = \"all\"\n",
    "\n",
    "# Ignore these directories during scans\n",
    "IGNORE_DIRS = {\"reports\", \"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\"}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def safe_mkdir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_csv_always(df: Optional[pd.DataFrame], path: Path, columns: List[str]):\n",
    "    safe_mkdir(path.parent)\n",
    "    if df is None or df.empty:\n",
    "        pd.DataFrame(columns=columns).to_csv(path, index=False)\n",
    "        return\n",
    "    out = df.copy()\n",
    "    for c in columns:\n",
    "        if c not in out.columns:\n",
    "            out[c] = \"\"\n",
    "    out[columns].to_csv(path, index=False)\n",
    "\n",
    "def normalize_label(x: str) -> str:\n",
    "    x = str(x).strip().upper()\n",
    "    if x == \"HEALTHY\":\n",
    "        return \"NORMAL\"\n",
    "    return x\n",
    "\n",
    "def normalize_split(s: str) -> str:\n",
    "    s = (s or \"\").lower().strip()\n",
    "    return SPLIT_CANON.get(s, s)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def stable_mode(items: List[str]) -> str:\n",
    "    c = Counter(items)\n",
    "    max_ct = max(c.values())\n",
    "    winners = sorted([k for k, v in c.items() if v == max_ct])\n",
    "    return winners[0]\n",
    "\n",
    "def detect_has_splits(root: Path) -> bool:\n",
    "    if not root.exists():\n",
    "        return False\n",
    "    kids = {p.name.lower() for p in root.iterdir() if p.is_dir()}\n",
    "    return any(s in kids for s in {\"train\", \"val\", \"test\", \"valid\", \"validation\"})\n",
    "\n",
    "def infer_split_label_from_path(dataset_root: Path, img_path: Path) -> Tuple[str, str, Path]:\n",
    "    rel = img_path.relative_to(dataset_root)\n",
    "    parts = rel.parts\n",
    "\n",
    "    split = \"unknown\"\n",
    "    split_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        pl = part.lower()\n",
    "        if pl in SPLITS:\n",
    "            split = normalize_split(pl)\n",
    "            split_idx = i\n",
    "            break\n",
    "\n",
    "    if split_idx is not None and split_idx + 1 < len(parts):\n",
    "        label = normalize_label(parts[split_idx + 1])\n",
    "        return split, label, rel\n",
    "\n",
    "    label = normalize_label(parts[0]) if len(parts) >= 1 else \"UNKNOWN\"\n",
    "    return \"unknown\", label, rel\n",
    "\n",
    "def iter_images_scan(root: Path):\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if p.suffix.lower() not in VALID_EXT:\n",
    "            continue\n",
    "        if any(part in IGNORE_DIRS for part in p.parts):\n",
    "            continue\n",
    "        yield p\n",
    "\n",
    "def canonical_sort(df_group: pd.DataFrame, has_splits: bool) -> pd.DataFrame:\n",
    "    g = df_group.copy()\n",
    "    if has_splits:\n",
    "        g[\"prio\"] = g[\"split\"].map(lambda s: SPLIT_PRIORITY.get(str(s), 3))\n",
    "        return g.sort_values([\"prio\", \"rel_path\"], ascending=True)\n",
    "    return g.sort_values([\"path\"], ascending=True)\n",
    "\n",
    "def place_file(src: Path, dst: Path):\n",
    "    safe_mkdir(dst.parent)\n",
    "    if dst.exists():\n",
    "        return\n",
    "    if COPY_MODE == \"copy\":\n",
    "        shutil.copy2(str(src), str(dst))\n",
    "    elif COPY_MODE == \"hardlink\":\n",
    "        os.link(str(src), str(dst))\n",
    "    elif COPY_MODE == \"symlink\":\n",
    "        os.symlink(str(src), str(dst))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown COPY_MODE: {COPY_MODE}\")\n",
    "\n",
    "def detect_extensions_from_disk(source_root: Path, class_names: List[str]) -> List[str]:\n",
    "    exts = set()\n",
    "    for c in class_names:\n",
    "        d = source_root / c\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for p in d.iterdir():\n",
    "            if p.is_file():\n",
    "                exts.add(p.suffix.lower())\n",
    "    if not exts:\n",
    "        exts = set(VALID_EXT)\n",
    "    return sorted(exts)\n",
    "\n",
    "def resolve_octdl_style_path(source_root: Path, disease: str, file_name: str, exts: List[str]) -> Optional[Path]:\n",
    "    disease = normalize_label(disease)\n",
    "    cls_dir = source_root / disease\n",
    "    if not cls_dir.exists():\n",
    "        return None\n",
    "\n",
    "    fn = str(file_name).strip()\n",
    "    p = Path(fn)\n",
    "\n",
    "    if p.suffix:\n",
    "        cand = cls_dir / fn\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "        matches = list(cls_dir.glob(p.name))\n",
    "        return matches[0] if len(matches) == 1 else None\n",
    "\n",
    "    for ext in exts:\n",
    "        cand = cls_dir / f\"{fn}{ext}\"\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "\n",
    "    matches = list(cls_dir.glob(f\"{fn}.*\"))\n",
    "    return matches[0] if len(matches) == 1 else None\n",
    "\n",
    "def guess_csv_profile(df: pd.DataFrame) -> str:\n",
    "    cols = set(df.columns)\n",
    "    if {\"Patient ID\", \"Class\", \"Label\", \"Directory\"}.issubset(cols):\n",
    "        return \"NEH\"\n",
    "    if {\"file_name\", \"disease\", \"patient_id\"}.issubset(set(map(str.lower, df.columns))):\n",
    "        return \"OCTDL\"\n",
    "    return \"GENERIC\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Stats helpers (BEFORE/AFTER reporting)\n",
    "# ============================================================\n",
    "\n",
    "def summarize_manifest(m: pd.DataFrame, title: str):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"[STATS] {title}\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    total = len(m)\n",
    "    print(f\"Total images: {total:,}\")\n",
    "\n",
    "    if \"split\" in m.columns:\n",
    "        split_counts = m[\"split\"].fillna(\"unknown\").astype(str).value_counts()\n",
    "        print(\"\\nImages per split:\")\n",
    "        print(split_counts.to_string())\n",
    "\n",
    "    if \"label\" in m.columns:\n",
    "        label_counts = m[\"label\"].fillna(\"UNKNOWN\").astype(str).value_counts()\n",
    "        print(\"\\nImages per class:\")\n",
    "        print(label_counts.to_string())\n",
    "\n",
    "    if \"split\" in m.columns and \"label\" in m.columns:\n",
    "        pivot = (\n",
    "            m.assign(split=m[\"split\"].fillna(\"unknown\").astype(str),\n",
    "                     label=m[\"label\"].fillna(\"UNKNOWN\").astype(str))\n",
    "             .groupby([\"split\",\"label\"]).size()\n",
    "             .unstack(fill_value=0)\n",
    "        )\n",
    "        print(\"\\nImages per split x class:\")\n",
    "        print(pivot)\n",
    "\n",
    "    if \"patient_uid\" in m.columns:\n",
    "        pu = m[\"patient_uid\"].astype(str).str.strip()\n",
    "        if pu.ne(\"\").any():\n",
    "            print(f\"\\nUnique patients (patient_uid): {pu[pu.ne('')].nunique():,}\")\n",
    "        else:\n",
    "            print(\"\\nUnique patients (patient_uid): N/A (not provided)\")\n",
    "\n",
    "def write_stats_csvs(m: pd.DataFrame, out_dir: Path, prefix: str):\n",
    "    safe_mkdir(out_dir)\n",
    "    m2 = m.copy()\n",
    "    m2[\"split\"] = m2.get(\"split\", \"unknown\")\n",
    "    m2[\"label\"] = m2.get(\"label\", \"UNKNOWN\")\n",
    "\n",
    "    split_counts = m2[\"split\"].astype(str).value_counts().reset_index()\n",
    "    split_counts.columns = [\"split\", \"count\"]\n",
    "    split_counts.to_csv(out_dir / f\"{prefix}_counts_by_split.csv\", index=False)\n",
    "\n",
    "    label_counts = m2[\"label\"].astype(str).value_counts().reset_index()\n",
    "    label_counts.columns = [\"label\", \"count\"]\n",
    "    label_counts.to_csv(out_dir / f\"{prefix}_counts_by_label.csv\", index=False)\n",
    "\n",
    "    pivot = (\n",
    "        m2.groupby([\"split\",\"label\"]).size().reset_index(name=\"count\")\n",
    "    )\n",
    "    pivot.to_csv(out_dir / f\"{prefix}_counts_by_split_label.csv\", index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Manifest builders\n",
    "# ============================================================\n",
    "\n",
    "def build_manifest_from_csv(root_dir: Path, csv_path: Path, has_splits: bool) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    profile = CSV_PROFILE\n",
    "    if profile == \"auto\":\n",
    "        profile = guess_csv_profile(df)\n",
    "\n",
    "    rows, bad = [], []\n",
    "\n",
    "    if profile == \"NEH\":\n",
    "        required = {\"Patient ID\", \"Class\", \"Label\", \"Directory\"}\n",
    "        missing = required - set(df.columns)\n",
    "        if missing:\n",
    "            raise RuntimeError(f\"NEH CSV missing columns: {missing}\")\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"Label\"] = df[\"Label\"].apply(normalize_label)\n",
    "        df[\"Class\"] = df[\"Class\"].apply(normalize_label)\n",
    "\n",
    "        if FILTER_MODE.lower() == \"worstcase\":\n",
    "            df = df[df[\"Class\"] == df[\"Label\"]].copy()\n",
    "\n",
    "        for _, r in df.iterrows():\n",
    "            rel = str(r[\"Directory\"]).lstrip(\"/\\\\\")\n",
    "            img_path = root_dir / rel\n",
    "            if img_path.suffix.lower() not in VALID_EXT:\n",
    "                continue\n",
    "            if not img_path.exists():\n",
    "                bad.append({\"row\": rel, \"error\": \"missing_on_disk\"})\n",
    "                continue\n",
    "\n",
    "            patient_id = str(r[\"Patient ID\"]).strip()\n",
    "            patient_class = str(r[\"Class\"]).strip()\n",
    "            patient_uid = f\"{patient_class}_{patient_id}\"\n",
    "\n",
    "            split = \"unknown\"\n",
    "            if has_splits:\n",
    "                split, _, _rel2 = infer_split_label_from_path(root_dir, img_path)\n",
    "                rel_path = str(_rel2)\n",
    "            else:\n",
    "                rel_path = str(img_path.relative_to(root_dir))\n",
    "\n",
    "            rows.append({\n",
    "                \"path\": str(img_path),\n",
    "                \"rel_path\": rel_path,\n",
    "                \"label\": str(r[\"Label\"]),\n",
    "                \"split\": split,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"patient_uid\": patient_uid,\n",
    "                \"patient_class\": patient_class,\n",
    "                \"source\": \"csv(NEH)\",\n",
    "            })\n",
    "\n",
    "    elif profile == \"OCTDL\":\n",
    "        cols_lower = {c.lower(): c for c in df.columns}\n",
    "        need = {\"file_name\", \"disease\", \"patient_id\"}\n",
    "        if not need.issubset(set(cols_lower.keys())):\n",
    "            raise RuntimeError(f\"OCTDL CSV missing columns. Need {need}, got {set(cols_lower.keys())}\")\n",
    "\n",
    "        fn_col = cols_lower[\"file_name\"]\n",
    "        dis_col = cols_lower[\"disease\"]\n",
    "        pid_col = cols_lower[\"patient_id\"]\n",
    "\n",
    "        df = df.copy()\n",
    "        df[dis_col] = df[dis_col].apply(normalize_label)\n",
    "        df[fn_col] = df[fn_col].astype(str).str.strip()\n",
    "        df[pid_col] = df[pid_col].astype(str).str.strip()\n",
    "\n",
    "        class_names = sorted(df[dis_col].unique().tolist())\n",
    "        exts = detect_extensions_from_disk(root_dir, class_names)\n",
    "\n",
    "        for r in df.itertuples(index=False):\n",
    "            disease = normalize_label(getattr(r, dis_col))\n",
    "            file_name = str(getattr(r, fn_col))\n",
    "            patient_id = str(getattr(r, pid_col))\n",
    "\n",
    "            img_path = resolve_octdl_style_path(root_dir, disease, file_name, exts)\n",
    "            if img_path is None or not img_path.exists():\n",
    "                bad.append({\"row\": f\"{disease}/{file_name}\", \"error\": \"missing_or_unresolved\"})\n",
    "                continue\n",
    "            if img_path.suffix.lower() not in VALID_EXT:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"path\": str(img_path),\n",
    "                \"rel_path\": str(img_path.relative_to(root_dir)),\n",
    "                \"label\": disease,\n",
    "                \"split\": \"unknown\",\n",
    "                \"patient_id\": patient_id,\n",
    "                \"patient_uid\": patient_id,\n",
    "                \"patient_class\": \"\",\n",
    "                \"source\": \"csv(OCTDL)\",\n",
    "            })\n",
    "\n",
    "    else:\n",
    "        cols = list(df.columns)\n",
    "        cols_lower = {c.lower(): c for c in cols}\n",
    "\n",
    "        path_col = None\n",
    "        for cand in [\"path\", \"file_path\", \"filepath\", \"image_path\", \"directory\", \"file\", \"filename\", \"file_name\"]:\n",
    "            if cand in cols_lower:\n",
    "                path_col = cols_lower[cand]\n",
    "                break\n",
    "\n",
    "        label_col = None\n",
    "        for cand in [\"label\", \"class\", \"disease\", \"diagnosis\", \"category\"]:\n",
    "            if cand in cols_lower:\n",
    "                label_col = cols_lower[cand]\n",
    "                break\n",
    "\n",
    "        patient_col = None\n",
    "        for cand in [\"patient_id\", \"patient id\", \"subject_id\", \"subject id\", \"case_id\", \"case id\"]:\n",
    "            if cand in cols_lower:\n",
    "                patient_col = cols_lower[cand]\n",
    "                break\n",
    "\n",
    "        if path_col is None or label_col is None:\n",
    "            raise RuntimeError(f\"GENERIC CSV needs path+label columns. Got columns: {cols}\")\n",
    "\n",
    "        for _, r in df.iterrows():\n",
    "            raw_path = str(r[path_col]).strip()\n",
    "            label = normalize_label(str(r[label_col]))\n",
    "\n",
    "            img_path = Path(raw_path)\n",
    "            if not img_path.is_absolute():\n",
    "                img_path = root_dir / raw_path\n",
    "\n",
    "            if img_path.suffix.lower() not in VALID_EXT:\n",
    "                continue\n",
    "            if not img_path.exists():\n",
    "                bad.append({\"row\": raw_path, \"error\": \"missing_on_disk\"})\n",
    "                continue\n",
    "\n",
    "            pid = str(r[patient_col]).strip() if patient_col else \"\"\n",
    "            patient_uid = pid if pid else \"\"\n",
    "\n",
    "            split = \"unknown\"\n",
    "            if has_splits:\n",
    "                split, _, rel = infer_split_label_from_path(root_dir, img_path)\n",
    "                rel_path = str(rel)\n",
    "            else:\n",
    "                rel_path = str(img_path.relative_to(root_dir))\n",
    "\n",
    "            rows.append({\n",
    "                \"path\": str(img_path),\n",
    "                \"rel_path\": rel_path,\n",
    "                \"label\": label,\n",
    "                \"split\": split,\n",
    "                \"patient_id\": pid,\n",
    "                \"patient_uid\": patient_uid,\n",
    "                \"patient_class\": \"\",\n",
    "                \"source\": \"csv(GENERIC)\",\n",
    "            })\n",
    "\n",
    "    manifest = pd.DataFrame(rows).drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
    "    bad_df = pd.DataFrame(bad)\n",
    "    return manifest, bad_df\n",
    "\n",
    "def build_manifest_by_scanning(root_dir: Path, has_splits: bool) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for p in iter_images_scan(root_dir):\n",
    "        split, label, rel = infer_split_label_from_path(root_dir, p)\n",
    "\n",
    "        patient_id = \"\"\n",
    "        patient_uid = \"\"\n",
    "        patient_class = \"\"\n",
    "\n",
    "        if not has_splits:\n",
    "            rel_parts = rel.parts\n",
    "            if len(rel_parts) >= 2:\n",
    "                patient_id = str(rel_parts[1])\n",
    "                patient_uid = patient_id\n",
    "                patient_class = label\n",
    "\n",
    "        rows.append({\n",
    "            \"path\": str(p),\n",
    "            \"rel_path\": str(rel),\n",
    "            \"label\": label,\n",
    "            \"split\": split if has_splits else \"unknown\",\n",
    "            \"patient_id\": patient_id,\n",
    "            \"patient_uid\": patient_uid,\n",
    "            \"patient_class\": patient_class,\n",
    "            \"source\": \"scan\",\n",
    "        })\n",
    "    return pd.DataFrame(rows).drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dedupe (SHA-256 ONLY)\n",
    "# ============================================================\n",
    "\n",
    "def dedupe_sha256(manifest: pd.DataFrame, has_splits: bool, reports_dir: Path) -> Tuple[pd.DataFrame, Dict[str,int]]:\n",
    "    print(\"[INFO] Computing SHA-256 (exact duplicates only)...\")\n",
    "    sha_list = []\n",
    "    bad = []\n",
    "\n",
    "    for p in tqdm(manifest[\"path\"].tolist(), desc=\"sha256\", unit=\"img\"):\n",
    "        try:\n",
    "            sha_list.append(sha256_file(Path(p)))\n",
    "        except Exception as e:\n",
    "            sha_list.append(None)\n",
    "            bad.append({\"path\": p, \"stage\": \"sha256\", \"error\": str(e)})\n",
    "\n",
    "    m = manifest.copy()\n",
    "    m[\"sha256\"] = sha_list\n",
    "    m = m.dropna(subset=[\"sha256\"]).reset_index(drop=True)\n",
    "\n",
    "    write_csv_always(pd.DataFrame(bad), reports_dir / \"bad_files_hashing.csv\",\n",
    "                     [\"path\", \"stage\", \"error\"])\n",
    "\n",
    "    conflict_rows = []\n",
    "    removed_rows = []\n",
    "    keep_rows = []\n",
    "\n",
    "    dup_groups = 0\n",
    "    removed_dupes = 0\n",
    "    removed_conflicts = 0\n",
    "\n",
    "    for sha, g in m.groupby(\"sha256\", sort=False):\n",
    "        if len(g) == 1:\n",
    "            keep_rows.append(g.iloc[0].to_dict())\n",
    "            continue\n",
    "\n",
    "        dup_groups += 1\n",
    "        labels = set(g[\"label\"].tolist())\n",
    "\n",
    "        # If same bytes map to multiple labels => drop all (conservative)\n",
    "        if len(labels) > 1:\n",
    "            removed_conflicts += len(g)\n",
    "            for _, rr in g.iterrows():\n",
    "                conflict_rows.append({**rr.to_dict(), \"reason\": \"sha256_label_conflict_drop_all\"})\n",
    "            continue\n",
    "\n",
    "        patients = set([x for x in g[\"patient_uid\"].tolist() if str(x).strip() != \"\"])\n",
    "        if DROP_CROSS_PATIENT_EXACT_DUPES and len(patients) > 1:\n",
    "            removed_conflicts += len(g)\n",
    "            for _, rr in g.iterrows():\n",
    "                removed_rows.append({\n",
    "                    \"sha256\": sha,\n",
    "                    \"canonical_path\": \"\",\n",
    "                    \"removed_path\": rr[\"path\"],\n",
    "                    \"canonical_split\": \"\",\n",
    "                    \"removed_split\": rr[\"split\"],\n",
    "                    \"label\": rr[\"label\"],\n",
    "                    \"reason\": \"sha256_cross_patient_drop_all\",\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        g_sorted = canonical_sort(g, has_splits)\n",
    "        canon = g_sorted.iloc[0].to_dict()\n",
    "        keep_rows.append(canon)\n",
    "\n",
    "        for i in range(1, len(g_sorted)):\n",
    "            rr = g_sorted.iloc[i].to_dict()\n",
    "            removed_dupes += 1\n",
    "            removed_rows.append({\n",
    "                \"sha256\": sha,\n",
    "                \"canonical_path\": canon[\"path\"],\n",
    "                \"removed_path\": rr[\"path\"],\n",
    "                \"canonical_split\": canon.get(\"split\", \"unknown\"),\n",
    "                \"removed_split\": rr.get(\"split\", \"unknown\"),\n",
    "                \"label\": canon[\"label\"],\n",
    "                \"reason\": \"sha256_exact_duplicate_keep_one\",\n",
    "            })\n",
    "\n",
    "    kept = pd.DataFrame(keep_rows).reset_index(drop=True)\n",
    "\n",
    "    write_csv_always(pd.DataFrame(conflict_rows), reports_dir / \"removed_conflicts_sha256.csv\",\n",
    "                     [\"path\",\"rel_path\",\"split\",\"label\",\"patient_uid\",\"sha256\",\"reason\",\"source\"])\n",
    "    write_csv_always(pd.DataFrame(removed_rows), reports_dir / \"removed_duplicates_sha256.csv\",\n",
    "                     [\"sha256\",\"canonical_path\",\"removed_path\",\"canonical_split\",\"removed_split\",\"label\",\"reason\"])\n",
    "    write_csv_always(kept, reports_dir / \"manifest_after_sha256.csv\",\n",
    "                     [\"path\",\"rel_path\",\"split\",\"label\",\"patient_uid\",\"patient_id\",\"patient_class\",\"sha256\",\"source\"])\n",
    "\n",
    "    stats = {\n",
    "        \"bad_hash_reads\": len(bad),\n",
    "        \"duplicate_groups_sha256\": dup_groups,\n",
    "        \"removed_duplicates_sha256\": removed_dupes,\n",
    "        \"removed_conflicts_sha256\": removed_conflicts,\n",
    "        \"kept_after_sha256\": len(kept),\n",
    "    }\n",
    "\n",
    "    print(f\"[INFO] After SHA-256: kept {len(kept):,} images\")\n",
    "    print(\"[INFO] SHA-256 dedupe stats:\", stats)\n",
    "    return kept, stats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Splitting + Materialization\n",
    "# ============================================================\n",
    "\n",
    "def assign_splits(m: pd.DataFrame, has_splits: bool, reports_dir: Path) -> pd.DataFrame:\n",
    "    if has_splits:\n",
    "        out = m.copy()\n",
    "        out[\"split\"] = out[\"split\"].fillna(\"unknown\").map(lambda s: normalize_split(str(s)))\n",
    "        return out\n",
    "\n",
    "    out = m.copy()\n",
    "    have_patient = out[\"patient_uid\"].astype(str).str.strip().ne(\"\").any()\n",
    "\n",
    "    if have_patient:\n",
    "        patient_primary = (\n",
    "            out.groupby(\"patient_uid\")[\"label\"]\n",
    "               .apply(lambda s: stable_mode(s.tolist()))\n",
    "               .to_dict()\n",
    "        )\n",
    "        out[\"patient_class\"] = out[\"patient_uid\"].map(patient_primary)\n",
    "\n",
    "        patient_df = pd.DataFrame({\n",
    "            \"patient_uid\": list(patient_primary.keys()),\n",
    "            \"patient_class\": list(patient_primary.values()),\n",
    "        })\n",
    "\n",
    "        patients = patient_df[\"patient_uid\"].tolist()\n",
    "        strat = patient_df[\"patient_class\"].tolist()\n",
    "\n",
    "        counts = pd.Series(strat).value_counts()\n",
    "        can_stratify = (len(counts) >= 2) and (counts.min() >= 2)\n",
    "        if not can_stratify:\n",
    "            print(\"[WARN] Not enough patients per class for stratify; falling back to non-stratified patient split.\")\n",
    "            strat = None\n",
    "\n",
    "        train_pat, temp_pat = train_test_split(\n",
    "            patients,\n",
    "            test_size=(1.0 - SPLIT_RATIOS[\"train\"]),\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=strat\n",
    "        )\n",
    "\n",
    "        val_ratio = SPLIT_RATIOS[\"val\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"])\n",
    "        if strat is not None:\n",
    "            strat_map = dict(zip(patients, patient_df[\"patient_class\"].tolist()))\n",
    "            temp_strat = [strat_map[p] for p in temp_pat]\n",
    "        else:\n",
    "            temp_strat = None\n",
    "\n",
    "        val_pat, test_pat = train_test_split(\n",
    "            temp_pat,\n",
    "            test_size=(1.0 - val_ratio),\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=temp_strat\n",
    "        )\n",
    "\n",
    "        split_map = {pid: \"train\" for pid in train_pat}\n",
    "        split_map.update({pid: \"val\" for pid in val_pat})\n",
    "        split_map.update({pid: \"test\" for pid in test_pat})\n",
    "\n",
    "        out[\"split\"] = out[\"patient_uid\"].map(split_map)\n",
    "\n",
    "        assert out[\"split\"].isna().sum() == 0, \"Some rows did not get a split assignment.\"\n",
    "        assert out.groupby(\"patient_uid\")[\"split\"].nunique().max() == 1, \"Patient leakage detected.\"\n",
    "\n",
    "        pat_summary = (\n",
    "            out[[\"patient_uid\", \"split\", \"patient_class\"]]\n",
    "            .drop_duplicates()\n",
    "            .groupby([\"split\", \"patient_class\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "        )\n",
    "        pat_summary.to_csv(reports_dir / \"split_summary_patients.csv\")\n",
    "        print(\"[INFO] Patient-wise split done (no leakage).\")\n",
    "        return out\n",
    "\n",
    "    print(\"[WARN] No patient IDs available; doing IMAGE-wise stratified split (leakage possible).\")\n",
    "    labels = out[\"label\"].tolist()\n",
    "    idxs = list(range(len(out)))\n",
    "\n",
    "    counts = pd.Series(labels).value_counts()\n",
    "    can_stratify = (len(counts) >= 2) and (counts.min() >= 2)\n",
    "    strat = labels if can_stratify else None\n",
    "\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        idxs,\n",
    "        test_size=(1.0 - SPLIT_RATIOS[\"train\"]),\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=strat\n",
    "    )\n",
    "\n",
    "    val_ratio = SPLIT_RATIOS[\"val\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"])\n",
    "\n",
    "    if strat is not None:\n",
    "        temp_labels = [labels[i] for i in temp_idx]\n",
    "    else:\n",
    "        temp_labels = None\n",
    "\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=(1.0 - val_ratio),\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=temp_labels\n",
    "    )\n",
    "\n",
    "    split = [\"\"] * len(out)\n",
    "    for i in train_idx: split[i] = \"train\"\n",
    "    for i in val_idx:   split[i] = \"val\"\n",
    "    for i in test_idx:  split[i] = \"test\"\n",
    "    out[\"split\"] = split\n",
    "    return out\n",
    "\n",
    "def materialize(m: pd.DataFrame, out_dir: Path, reports_dir: Path):\n",
    "    print(\"[INFO] Writing output dataset...\")\n",
    "    safe_mkdir(out_dir)\n",
    "    safe_mkdir(reports_dir)\n",
    "\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "    labels = sorted(set(m[\"label\"].tolist()))\n",
    "    for sp in splits:\n",
    "        for lab in labels:\n",
    "            safe_mkdir(out_dir / sp / lab)\n",
    "\n",
    "    out_paths = []\n",
    "    bad_writes = []\n",
    "\n",
    "    for _, r in tqdm(m.iterrows(), total=len(m), desc=\"writing\", unit=\"img\"):\n",
    "        src = Path(r[\"path\"])\n",
    "        split = str(r[\"split\"])\n",
    "        label = str(r[\"label\"])\n",
    "\n",
    "        sha8 = str(r.get(\"sha256\", \"\"))[:8] if pd.notna(r.get(\"sha256\", \"\")) else \"nosha\"\n",
    "        puid = str(r.get(\"patient_uid\", \"\")).strip()\n",
    "        puid_safe = puid.replace(\"/\", \"_\").replace(\"\\\\\", \"_\") if puid else \"\"\n",
    "\n",
    "        if puid_safe:\n",
    "            dst_name = f\"{puid_safe}__{sha8}__{src.name}\"\n",
    "        else:\n",
    "            dst_name = f\"{sha8}__{src.name}\"\n",
    "\n",
    "        dst = out_dir / split / label / dst_name\n",
    "\n",
    "        try:\n",
    "            place_file(src, dst)\n",
    "            out_paths.append(str(dst))\n",
    "        except Exception as e:\n",
    "            bad_writes.append({\"src\": str(src), \"dst\": str(dst), \"error\": str(e)})\n",
    "\n",
    "    m2 = m.copy().reset_index(drop=True)\n",
    "    m2[\"new_path\"] = out_paths[:len(m2)]\n",
    "\n",
    "    write_csv_always(pd.DataFrame(bad_writes), reports_dir / \"bad_writes.csv\",\n",
    "                     [\"src\", \"dst\", \"error\"])\n",
    "\n",
    "    m2.to_csv(out_dir / \"final_manifest.csv\", index=False)\n",
    "\n",
    "    img_summary = m2.groupby([\"split\", \"label\"]).size().unstack(fill_value=0)\n",
    "    img_summary.to_csv(out_dir / \"split_summary_images.csv\")\n",
    "\n",
    "    print(\"\\n[INFO] Images per split/label:\\n\", img_summary)\n",
    "    print(\"\\n[DONE] Output root:\", out_dir)\n",
    "    print(\"[DONE] Reports:\", reports_dir)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    assert abs(sum(SPLIT_RATIOS.values()) - 1.0) < 1e-9, \"Split ratios must sum to 1.0\"\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    safe_mkdir(OUT_DIR)\n",
    "    reports_dir = OUT_DIR / \"reports\"\n",
    "    safe_mkdir(reports_dir)\n",
    "\n",
    "    has_splits = detect_has_splits(ROOT_DIR)\n",
    "\n",
    "    if MODE == \"preserve_splits\":\n",
    "        has_splits = True\n",
    "    elif MODE == \"build_splits\":\n",
    "        has_splits = False\n",
    "    elif MODE != \"auto\":\n",
    "        raise ValueError(\"MODE must be one of: auto, preserve_splits, build_splits\")\n",
    "\n",
    "    print(\"[INFO] ROOT_DIR:\", ROOT_DIR)\n",
    "    print(\"[INFO] CSV_PATH:\", CSV_PATH)\n",
    "    print(\"[INFO] MODE:\", MODE, \"| detected_has_splits:\", has_splits)\n",
    "\n",
    "    # 1) Build manifest\n",
    "    bad_csv = pd.DataFrame()\n",
    "    if CSV_PATH is not None and Path(CSV_PATH).exists():\n",
    "        manifest, bad_csv = build_manifest_from_csv(ROOT_DIR, Path(CSV_PATH), has_splits)\n",
    "    else:\n",
    "        manifest = build_manifest_by_scanning(ROOT_DIR, has_splits)\n",
    "\n",
    "    if manifest.empty:\n",
    "        raise RuntimeError(\"No images found/resolved. Check ROOT_DIR/CSV_PATH and folder structure.\")\n",
    "\n",
    "    write_csv_always(manifest, reports_dir / \"manifest_raw.csv\",\n",
    "                     [\"path\",\"rel_path\",\"split\",\"label\",\"patient_uid\",\"patient_id\",\"patient_class\",\"source\"])\n",
    "    if not bad_csv.empty:\n",
    "        write_csv_always(bad_csv, reports_dir / \"csv_missing_or_unresolved.csv\", list(bad_csv.columns))\n",
    "        print(f\"[WARN] CSV unresolved rows: {len(bad_csv)} (see reports)\")\n",
    "\n",
    "    # ---- BEFORE stats ----\n",
    "    summarize_manifest(manifest, \"BEFORE DEDUPLICATION (raw manifest)\")\n",
    "    write_stats_csvs(manifest, reports_dir, \"before\")\n",
    "\n",
    "    # 2) SHA-256 exact dedupe ONLY\n",
    "    m, sha_stats = dedupe_sha256(manifest, has_splits=has_splits, reports_dir=reports_dir)\n",
    "\n",
    "    # 3) Assign / preserve splits\n",
    "    m = assign_splits(m, has_splits=has_splits, reports_dir=reports_dir)\n",
    "\n",
    "    # Drop unknown split rows (optional but keeps train/val/test clean)\n",
    "    unknown = m[\"split\"].astype(str).str.lower().eq(\"unknown\")\n",
    "    if unknown.any():\n",
    "        dropped = int(unknown.sum())\n",
    "        print(f\"[WARN] Dropping {dropped} rows with split='unknown' (path not under train/val/test).\")\n",
    "        write_csv_always(m[unknown], reports_dir / \"dropped_unknown_split.csv\",\n",
    "                         [\"path\",\"rel_path\",\"split\",\"label\",\"sha256\",\"source\"])\n",
    "        m = m[~unknown].copy().reset_index(drop=True)\n",
    "\n",
    "    # ---- AFTER stats (post-dedupe + split assignment) ----\n",
    "    summarize_manifest(m, \"AFTER SHA-256 DEDUPLICATION (and split assignment)\")\n",
    "    write_stats_csvs(m, reports_dir, \"after\")\n",
    "\n",
    "    # Summary line items\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"[SUMMARY] KEY NUMBERS\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"Before: {len(manifest):,} images\")\n",
    "    print(f\"After : {len(m):,} images\")\n",
    "    print(f\"Removed exact duplicates (kept one per SHA group): {sha_stats['removed_duplicates_sha256']:,}\")\n",
    "    print(f\"Removed label-conflict groups (dropped all in group): {sha_stats['removed_conflicts_sha256']:,}\")\n",
    "    print(f\"Bad/unreadable during hashing: {sha_stats['bad_hash_reads']:,}\")\n",
    "    print(f\"Duplicate groups found (SHA): {sha_stats['duplicate_groups_sha256']:,}\")\n",
    "    print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "    # 4) Materialize output dataset\n",
    "    materialize(m, OUT_DIR, reports_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5e499-53c9-4be3-babf-016fabe9ff94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
