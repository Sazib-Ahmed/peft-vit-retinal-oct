{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d7ba4f-ce54-4a4d-900d-25d71567b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 24,000 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|███████████████████████████████████████████████████████| 24000/24000 [02:54<00:00, 137.76img/s]\n",
      "Hashing (pHash-16): 100%|██████████████████████████████████████████████████████| 24000/24000 [01:25<00:00, 281.92img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "\n",
      "[1] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1002.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_2822.jpg\n",
      "\n",
      "[2] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1066.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1105.jpg\n",
      "\n",
      "[3] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1071.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1193.jpg\n",
      "\n",
      "[4] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1073.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1200.jpg\n",
      "\n",
      "[5] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1233.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_2311.jpg\n",
      "\n",
      "[6] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1239.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_2347.jpg\n",
      "\n",
      "[7] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1032.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DME\\dme_train_1221.jpg\n",
      "\n",
      "[8] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1044.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1045.jpg\n",
      "\n",
      "[9] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1149.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DME\\dme_train_1983.jpg\n",
      "\n",
      "[10] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1002.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_3159.jpg\n",
      "\n",
      "[11] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1127.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_1424.jpg\n",
      "\n",
      "[12] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1133.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_1478.jpg\n",
      "\n",
      "[13] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1249.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2487.jpg\n",
      "\n",
      "[14] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1251.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2518.jpg\n",
      "\n",
      "[15] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1252.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2526.jpg\n",
      "\n",
      "[16] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1264.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2649.jpg\n",
      "\n",
      "[17] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1276.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2717.jpg\n",
      "\n",
      "[18] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1277.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1278.jpg\n",
      "\n",
      "[19] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1283.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2750.jpg\n",
      "\n",
      "[20] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1296.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2815.jpg\n",
      "\n",
      "[21] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1297.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2823.jpg\n",
      "\n",
      "[22] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\NORMAL\\normal_test_1120.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\NORMAL\\normal_test_1121.jpg\n",
      "\n",
      "[23] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1002.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1003.jpg\n",
      "\n",
      "[24] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1055.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1056.jpg\n",
      "\n",
      "[25] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1175.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1178.jpg\n",
      "\n",
      "[26] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1180.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1181.jpg\n",
      "\n",
      "[27] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1183.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1184.jpg\n",
      "\n",
      "[28] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1189.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1190.jpg\n",
      "\n",
      "[29] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1213.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1214.jpg\n",
      "\n",
      "[30] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1281.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1282.jpg\n",
      "\n",
      "... (showing first 30 groups, 114 more not shown)\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "\n",
      "[1] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DME\\dme_train_2289.jpg\n",
      "    Duplicates found: 2\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DME\\dme_train_2290.jpg\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DME\\dme_train_2291.jpg\n",
      "\n",
      "[2] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1002.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_2822.jpg\n",
      "\n",
      "[3] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1066.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1105.jpg\n",
      "\n",
      "[4] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1071.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1193.jpg\n",
      "\n",
      "[5] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1073.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1200.jpg\n",
      "\n",
      "[6] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1233.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_2311.jpg\n",
      "\n",
      "[7] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\CNV\\cnv_test_1239.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_2347.jpg\n",
      "\n",
      "[8] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1032.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DME\\dme_train_1221.jpg\n",
      "\n",
      "[9] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1044.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1045.jpg\n",
      "\n",
      "[10] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DME\\dme_test_1149.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DME\\dme_train_1983.jpg\n",
      "\n",
      "[11] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1002.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_3159.jpg\n",
      "\n",
      "[12] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1127.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_1424.jpg\n",
      "\n",
      "[13] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1133.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_1478.jpg\n",
      "\n",
      "[14] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1249.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2487.jpg\n",
      "\n",
      "[15] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1251.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2518.jpg\n",
      "\n",
      "[16] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1252.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2526.jpg\n",
      "\n",
      "[17] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1264.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2649.jpg\n",
      "\n",
      "[18] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1276.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2717.jpg\n",
      "\n",
      "[19] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1277.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1278.jpg\n",
      "\n",
      "[20] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1283.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2750.jpg\n",
      "\n",
      "[21] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1296.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2815.jpg\n",
      "\n",
      "[22] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\DRUSEN\\drusen_test_1297.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test,train (cross_split=True)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\DRUSEN\\drusen_train_2823.jpg\n",
      "\n",
      "[23] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\NORMAL\\normal_test_1120.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: test (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\test\\NORMAL\\normal_test_1121.jpg\n",
      "\n",
      "[24] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1002.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1003.jpg\n",
      "\n",
      "[25] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1055.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1056.jpg\n",
      "\n",
      "[26] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1175.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1178.jpg\n",
      "\n",
      "[27] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1180.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1181.jpg\n",
      "\n",
      "[28] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1183.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1184.jpg\n",
      "\n",
      "[29] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1189.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1190.jpg\n",
      "\n",
      "[30] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1213.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: train (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\train\\CNV\\cnv_train_1214.jpg\n",
      "\n",
      "... (showing first 30 groups, 119 more not shown)\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 24,000\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 144 (cross-split groups: 48)\n",
      "Visual duplicate groups: 149 (cross-split groups: 48)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\\dup_reports\\duplicates_combined.csv\n",
      "      group_type                                          group_key  \\\n",
      "0   exact_sha256  6267eaf0c045fbc09c89aec6ee58702bc9334b3cc384f6...   \n",
      "1   exact_sha256  8c05d746bce2857607e48dfa08b09577572eb8b5354ad5...   \n",
      "2   exact_sha256  de528e87067088d5714680fbfe68d8879e929c03dbb859...   \n",
      "3   exact_sha256  25c3d47d9b3c85252ad0ca682c534532d21c31dc48b51e...   \n",
      "4   exact_sha256  a3d18049c58d5fadea0d9beb619047da62788bf24498cb...   \n",
      "5   exact_sha256  7c5ac984cb3d1c1e04131b23e8e9b5949c96d37de7d81d...   \n",
      "6   exact_sha256  567e46db6381d596f861aa15c94b7e89e6b1dc01a667a4...   \n",
      "7   exact_sha256  156128a34e8d34906d012e65ea38c89b4610b610b31d18...   \n",
      "8   exact_sha256  87b15ae51ae8d3944f395ca08b0aadcd1d9fb23574472c...   \n",
      "9   exact_sha256  0dc50c02037d04304cf9e9b763c3725945030af9b5b032...   \n",
      "10  exact_sha256  e7d6518a3412d5ae5aa9726c85c28f93454054ee6f3242...   \n",
      "11  exact_sha256  43151a6dd68b68d352160ff0cddc10713b6593aefb0b61...   \n",
      "12  exact_sha256  6c7719539cc02fe0b6755a0ae6da3d6305950b3f579141...   \n",
      "13  exact_sha256  e1bb6b5834098d7baf5d8ffa3c7c56527596953e60ea22...   \n",
      "14  exact_sha256  5da29b33bc2edcc987eeb064c808805dd4115530bc46eb...   \n",
      "15  exact_sha256  d79f48e2b18081cbfb99c54826b150385b2f78f052d661...   \n",
      "16  exact_sha256  750fe93031877e800b2657b785e449af18516ea1e3f405...   \n",
      "17  exact_sha256  75275ac9b202fa33af8424732500638447ddf42d1f4698...   \n",
      "18  exact_sha256  ec33615e340339bc73c57ccbe8d3f2701fb170849b4e9d...   \n",
      "19  exact_sha256  8a7d68f324f61243cf4825fdd427e51afd0d8ad7af1378...   \n",
      "\n",
      "                                       canonical_path  duplicate_count  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...                1   \n",
      "\n",
      "                                      duplicate_paths  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...   \n",
      "\n",
      "                                            all_paths splits_involved  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...            test   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...            test   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Datase...      test,train   \n",
      "\n",
      "    cross_split  \n",
      "0             1  \n",
      "1             1  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n",
      "5             1  \n",
      "6             1  \n",
      "7             0  \n",
      "8             1  \n",
      "9             1  \n",
      "10            1  \n",
      "11            1  \n",
      "12            1  \n",
      "13            1  \n",
      "14            1  \n",
      "15            1  \n",
      "16            1  \n",
      "17            0  \n",
      "18            1  \n",
      "19            1  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1832f80a-1bc7-4d8b-9558-fae8e95cc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 1,730 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|████████████████████████████████████████████████████████| 1730/1730 [00:00<00:00, 1832.14img/s]\n",
      "Hashing (pHash-16): 100%|████████████████████████████████████████████████████████| 1730/1730 [00:02<00:00, 777.94img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 1,730\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 0 (cross-split groups: 0)\n",
      "Visual duplicate groups: 0 (cross-split groups: 0)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\\dup_reports\\duplicates_combined.csv\n",
      "Empty DataFrame\n",
      "Columns: [group_type, group_key, canonical_path, duplicate_count, duplicate_paths, all_paths, splits_involved, cross_split]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\C8\\RetinalOCT_Dataset_CLEAN_SHAONLY\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80320b99-ce6e-4a35-bfef-e9197f2971a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
