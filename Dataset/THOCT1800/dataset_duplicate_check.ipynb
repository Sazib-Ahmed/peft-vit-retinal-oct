{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d7ba4f-ce54-4a4d-900d-25d71567b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 1,800 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|█████████████████████████████████████████████████████████| 1800/1800 [00:09<00:00, 199.89img/s]\n",
      "Hashing (pHash-16): 100%|████████████████████████████████████████████████████████| 1800/1800 [00:03<00:00, 498.93img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "\n",
      "[1] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_416.jpg\n",
      "    Duplicates found: 2\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_417.jpg\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_488.jpg\n",
      "\n",
      "[2] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_433.jpg\n",
      "    Duplicates found: 2\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_512.jpg\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_514.jpg\n",
      "\n",
      "[3] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_545.jpg\n",
      "    Duplicates found: 2\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_673.jpg\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_698.jpg\n",
      "\n",
      "[4] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_418.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_425.jpg\n",
      "\n",
      "[5] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_436.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_443.jpg\n",
      "\n",
      "[6] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_437.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_444.jpg\n",
      "\n",
      "[7] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_439.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_445.jpg\n",
      "\n",
      "[8] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_440.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_446.jpg\n",
      "\n",
      "[9] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_441.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_447.jpg\n",
      "\n",
      "[10] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_442.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_448.jpg\n",
      "\n",
      "[11] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_449.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_454.jpg\n",
      "\n",
      "[12] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_450.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_455.jpg\n",
      "\n",
      "[13] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_452.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_456.jpg\n",
      "\n",
      "[14] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_492.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_505.jpg\n",
      "\n",
      "[15] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_493.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_506.jpg\n",
      "\n",
      "[16] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_495.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_507.jpg\n",
      "\n",
      "[17] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_583.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_668.jpg\n",
      "\n",
      "[18] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_586.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_675.jpg\n",
      "\n",
      "[19] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_587.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_676.jpg\n",
      "\n",
      "[20] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_588.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_677.jpg\n",
      "\n",
      "[21] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_589.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_681.jpg\n",
      "\n",
      "[22] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_590.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_682.jpg\n",
      "\n",
      "[23] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_594.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_510.jpg\n",
      "\n",
      "[24] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_648.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_649.jpg\n",
      "\n",
      "[25] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_678.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_679.jpg\n",
      "\n",
      "[26] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_692.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_915.jpg\n",
      "\n",
      "[27] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_693.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_916.jpg\n",
      "\n",
      "[28] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_694.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_914.jpg\n",
      "\n",
      "[29] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_695.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_917.jpg\n",
      "\n",
      "[30] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_696.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_920.jpg\n",
      "\n",
      "... (showing first 30 groups, 36 more not shown)\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "\n",
      "[1] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_416.jpg\n",
      "    Duplicates found: 2\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_417.jpg\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_488.jpg\n",
      "\n",
      "[2] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_433.jpg\n",
      "    Duplicates found: 2\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_512.jpg\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_514.jpg\n",
      "\n",
      "[3] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_545.jpg\n",
      "    Duplicates found: 2\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_673.jpg\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_698.jpg\n",
      "\n",
      "[4] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_418.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_425.jpg\n",
      "\n",
      "[5] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_436.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_443.jpg\n",
      "\n",
      "[6] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_437.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_444.jpg\n",
      "\n",
      "[7] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_439.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_445.jpg\n",
      "\n",
      "[8] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_440.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_446.jpg\n",
      "\n",
      "[9] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_441.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_447.jpg\n",
      "\n",
      "[10] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_442.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_448.jpg\n",
      "\n",
      "[11] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_449.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_454.jpg\n",
      "\n",
      "[12] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_450.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_455.jpg\n",
      "\n",
      "[13] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_452.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_456.jpg\n",
      "\n",
      "[14] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_492.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_505.jpg\n",
      "\n",
      "[15] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_493.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_506.jpg\n",
      "\n",
      "[16] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_495.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_507.jpg\n",
      "\n",
      "[17] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_583.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_668.jpg\n",
      "\n",
      "[18] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_586.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_675.jpg\n",
      "\n",
      "[19] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_587.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_676.jpg\n",
      "\n",
      "[20] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_588.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_677.jpg\n",
      "\n",
      "[21] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_589.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_681.jpg\n",
      "\n",
      "[22] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_590.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_682.jpg\n",
      "\n",
      "[23] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_594.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\NOR\\aligned_nor_binary_cropped_510.jpg\n",
      "\n",
      "[24] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_648.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_649.jpg\n",
      "\n",
      "[25] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_678.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_679.jpg\n",
      "\n",
      "[26] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_692.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_915.jpg\n",
      "\n",
      "[27] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_693.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_916.jpg\n",
      "\n",
      "[28] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_694.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_914.jpg\n",
      "\n",
      "[29] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_695.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_917.jpg\n",
      "\n",
      "[30] Canonical: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_696.jpg\n",
      "    Duplicates found: 1\n",
      "    Splits involved: unknown (cross_split=False)\n",
      "      - D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\AMD\\aligned_amd_binary_cropped_920.jpg\n",
      "\n",
      "... (showing first 30 groups, 36 more not shown)\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 1,800\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 66 (cross-split groups: 0)\n",
      "Visual duplicate groups: 66 (cross-split groups: 0)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\dup_reports\\duplicates_combined.csv\n",
      "      group_type                                          group_key  \\\n",
      "0   exact_sha256  4868382d7191e11e62b1e101e7a674c471e323f440e412...   \n",
      "1   exact_sha256  dec51a4892c91c0ce6746ac8da8f85f71f20103af49a0f...   \n",
      "2   exact_sha256  ebda5025f14d9948ffe65dc9bc3d6cc3ee1e90254d4bf7...   \n",
      "3   exact_sha256  4df81d98573f401a85c5c9207f8a96279de4d2d43f17ab...   \n",
      "4   exact_sha256  ed3a17078ff6d96d8e5f6ebb4c19ee238ac4d84f83d7f8...   \n",
      "5   exact_sha256  5e8c4669f5f0460f1ff3670817d080cf55623ddc1ae747...   \n",
      "6   exact_sha256  b6a23ee6553804bf67b216d06b52b156c18b1910463529...   \n",
      "7   exact_sha256  5e19e9a62cd0bfa606cb355299905a1571445a73aea68e...   \n",
      "8   exact_sha256  10a9be91690ee4f0661ce730949f26457dcf49c555da3d...   \n",
      "9   exact_sha256  583599a4d856d50a53ce034d8206d1b2e83969d879cc40...   \n",
      "10  exact_sha256  9dab55bee5bb4c4e543e6e2cd5f0ac78b5f7b765617f75...   \n",
      "11  exact_sha256  7d419d5ab248be204c0e45b2589291b96cf19658c23902...   \n",
      "12  exact_sha256  dc4c6dd231ae45ff6bc7e576475ee1c27d65cf2c05356c...   \n",
      "13  exact_sha256  e780a2ceffb832891be263cb0054b8cfb198173c08fa6f...   \n",
      "14  exact_sha256  df66bb9146b2488550c3ab3e576ebae35481d360b79ee3...   \n",
      "15  exact_sha256  0e31af081378f4b810455998bff12d1f5c27b9244b7444...   \n",
      "16  exact_sha256  bad2c6d94a3230ab163dc251eacbf1942f78120050d8cc...   \n",
      "17  exact_sha256  4ce8eca8b3131b20222b99386ca627b8fae0640de7ac52...   \n",
      "18  exact_sha256  8af588b375950cd353ade56a5537d51986bbb45a15d71e...   \n",
      "19  exact_sha256  e355919cce190fa132485dcd0f0d20f0fc7f92c0d90b5c...   \n",
      "\n",
      "                                       canonical_path  duplicate_count  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                2   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                2   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                2   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...                1   \n",
      "\n",
      "                                      duplicate_paths  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...   \n",
      "\n",
      "                                            all_paths splits_involved  \\\n",
      "0   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "1   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "2   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "3   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "4   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "5   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "6   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "7   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "8   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "9   D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "10  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "11  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "12  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "13  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "14  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "15  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "16  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "17  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "18  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "19  D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\\...         unknown   \n",
      "\n",
      "    cross_split  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "5             0  \n",
      "6             0  \n",
      "7             0  \n",
      "8             0  \n",
      "9             0  \n",
      "10            0  \n",
      "11            0  \n",
      "12            0  \n",
      "13            0  \n",
      "14            0  \n",
      "15            0  \n",
      "16            0  \n",
      "17            0  \n",
      "18            0  \n",
      "19            0  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1832f80a-1bc7-4d8b-9558-fae8e95cc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 1,730 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|████████████████████████████████████████████████████████| 1730/1730 [00:00<00:00, 1832.14img/s]\n",
      "Hashing (pHash-16): 100%|████████████████████████████████████████████████████████| 1730/1730 [00:02<00:00, 777.94img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 1,730\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 0 (cross-split groups: 0)\n",
      "Visual duplicate groups: 0 (cross-split groups: 0)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\\dup_reports\\duplicates_combined.csv\n",
      "Empty DataFrame\n",
      "Columns: [group_type, group_key, canonical_path, duplicate_count, duplicate_paths, all_paths, splits_involved, cross_split]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80320b99-ce6e-4a35-bfef-e9197f2971a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train∩val: 0\n",
      "train∩test: 0\n",
      "val∩test: 0\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "VALID_EXT = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
    "\n",
    "def stems(split_dir):\n",
    "    out=set()\n",
    "    for p in glob.glob(os.path.join(split_dir,\"**\",\"*\"), recursive=True):\n",
    "        if os.path.isfile(p) and os.path.splitext(p)[1].lower() in VALID_EXT:\n",
    "            out.add(os.path.splitext(os.path.basename(p))[0])\n",
    "    return out\n",
    "\n",
    "root = r\"D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\"  # change\n",
    "tr = stems(os.path.join(root,\"train\"))\n",
    "va = stems(os.path.join(root,\"val\"))\n",
    "te = stems(os.path.join(root,\"test\"))\n",
    "\n",
    "print(\"train∩val:\", len(tr & va))\n",
    "print(\"train∩test:\", len(tr & te))\n",
    "print(\"val∩test:\", len(va & te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573d823c-e87c-43b8-831b-fd87d97f658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel-hash train∩val: 0\n",
      "pixel-hash train∩test: 0\n",
      "pixel-hash val∩test: 0\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "def img_pixel_hash(path, img_size=224, pad_to_square=True):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    if pad_to_square:\n",
    "        w,h = img.size\n",
    "        if w != h:\n",
    "            s = max(w,h)\n",
    "            bg = Image.new(\"RGB\",(s,s),(0,0,0))\n",
    "            bg.paste(img, ((s-w)//2,(s-h)//2))\n",
    "            img = bg\n",
    "    img = img.resize((img_size,img_size), Image.BILINEAR)\n",
    "    arr = np.array(img, dtype=np.uint8)\n",
    "    return hashlib.sha1(arr.tobytes()).hexdigest()\n",
    "\n",
    "def split_hashes(root, split):\n",
    "    hs=set()\n",
    "    for p in glob.glob(os.path.join(root, split, \"**\", \"*\"), recursive=True):\n",
    "        if os.path.isfile(p) and os.path.splitext(p)[1].lower() in VALID_EXT:\n",
    "            hs.add(img_pixel_hash(p))\n",
    "    return hs\n",
    "\n",
    "root = r\"D:\\AIUB\\DSP\\Code\\Datasets\\THOCT1800\\THOCT1800_CLEAN_SHAONLY\"  # change\n",
    "Htr = split_hashes(root,\"train\")\n",
    "Hva = split_hashes(root,\"val\")\n",
    "Hte = split_hashes(root,\"test\")\n",
    "\n",
    "print(\"pixel-hash train∩val:\", len(Htr & Hva))\n",
    "print(\"pixel-hash train∩test:\", len(Htr & Hte))\n",
    "print(\"pixel-hash val∩test:\", len(Hva & Hte))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94e0d1-c4ff-4933-86e1-ea5dbfa1c2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
