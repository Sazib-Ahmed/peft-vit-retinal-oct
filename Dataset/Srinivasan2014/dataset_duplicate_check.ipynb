{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d7ba4f-ce54-4a4d-900d-25d71567b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 3,231 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|█████████████████████████████████████████████████████████| 3231/3231 [00:18<00:00, 172.94img/s]\n",
      "Hashing (pHash-16): 100%|████████████████████████████████████████████████████████| 3231/3231 [00:08<00:00, 397.29img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 3,231\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 0 (cross-split groups: 0)\n",
      "Visual duplicate groups: 0 (cross-split groups: 0)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014\\dup_reports\\duplicates_combined.csv\n",
      "Empty DataFrame\n",
      "Columns: [group_type, group_key, canonical_path, duplicate_count, duplicate_paths, all_paths, splits_involved, cross_split]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1832f80a-1bc7-4d8b-9558-fae8e95cc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 3,231 image files under: D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014_CLEAN_SHAONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing (SHA-256): 100%|████████████████████████████████████████████████████████| 3231/3231 [00:02<00:00, 1434.37img/s]\n",
      "Hashing (pHash-16): 100%|████████████████████████████████████████████████████████| 3231/3231 [00:07<00:00, 411.94img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[REPORT] EXACT duplicates (same bytes / SHA-256)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[REPORT] VISUAL duplicates (pHash hash_size=16)\n",
      "==========================================================================================\n",
      "No groups found.\n",
      "\n",
      "==========================================================================================\n",
      "[SUMMARY]\n",
      "Total images scanned: 3,231\n",
      "Bad/unreadable images: 0\n",
      "Exact duplicate groups: 0 (cross-split groups: 0)\n",
      "Visual duplicate groups: 0 (cross-split groups: 0)\n",
      "Saved reports to: D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014_CLEAN_SHAONLY\\dup_reports\n",
      "==========================================================================================\n",
      "\n",
      "[INFO] Preview: D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014_CLEAN_SHAONLY\\dup_reports\\duplicates_combined.csv\n",
      "Empty DataFrame\n",
      "Columns: [group_type, group_key, canonical_path, duplicate_count, duplicate_paths, all_paths, splits_involved, cross_split]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Duplicate Image Scanner (EXACT bytes hash + VISUAL pHash)\n",
    "# - Recursively scans ALL subfolders under a dataset root\n",
    "# - EXACT duplicates: SHA-256 of file bytes (robust to filename)\n",
    "# - VISUAL duplicates: perceptual hash (pHash) of decoded image\n",
    "# - Prints a detailed report + saves CSV reports\n",
    "# - FIX: Always write CSVs with headers (even if empty) \n",
    "# ============================================================\n",
    "\n",
    "import os, sys, hashlib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Auto-install dependencies (Jupyter-friendly)\n",
    "# -----------------------------\n",
    "def _ensure_packages():\n",
    "    import importlib, subprocess\n",
    "    pkgs = [\n",
    "        (\"PIL\", \"pillow\"),\n",
    "        (\"imagehash\", \"imagehash\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"tqdm\", \"tqdm\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            missing.append(pip_name)\n",
    "    if missing:\n",
    "        print(\"[INFO] Installing missing packages:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit these)\n",
    "# -----------------------------\n",
    "DATASET_ROOT = r\"D:\\AIUB\\DSP\\Code\\Datasets\\Srinivasan_2014\\Srinivasan_2014_CLEAN_SHAONLY\"\n",
    "DO_VISUAL = True          # pHash check (slower but catches re-encoded duplicates)\n",
    "PHASH_BITS = 16           # hash_size for pHash (16 = strong, 8 = faster)\n",
    "PRINT_MAX_GROUPS = 30     # how many groups to print in console\n",
    "IGNORE_DIRS = (\"dup_reports\", \".git\", \"__pycache__\", \".ipynb_checkpoints\")\n",
    "REPORT_DIR = None         # None -> <dataset_root>/dup_reports\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".gif\", \".webp\"}\n",
    "\n",
    "def iter_image_files(root: str, ignore_dirs=None):\n",
    "    root = Path(root)\n",
    "    ignore_dirs = set(ignore_dirs or [])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            if any(part in ignore_dirs for part in p.parts):\n",
    "                continue\n",
    "            yield p\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def phash_image(path: Path, hash_size: int = 16) -> str:\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return str(imagehash.phash(im, hash_size=hash_size))\n",
    "\n",
    "def guess_split(dataset_root: Path, file_path: Path):\n",
    "    rel = file_path.relative_to(dataset_root)\n",
    "    for part in rel.parts:\n",
    "        pl = part.lower()\n",
    "        if pl in {\"train\", \"val\", \"valid\", \"validation\", \"test\"}:\n",
    "            return pl\n",
    "    return \"unknown\"\n",
    "\n",
    "def _group_to_row(group_type: str, key: str, paths: list, dataset_root: Path):\n",
    "    paths_sorted = sorted([str(p) for p in paths], key=lambda s: (len(s), s))\n",
    "    canonical = paths_sorted[0]\n",
    "    dupes = paths_sorted[1:]\n",
    "    splits = sorted({guess_split(dataset_root, Path(p)) for p in paths_sorted})\n",
    "    return {\n",
    "        \"group_type\": group_type,\n",
    "        \"group_key\": key,\n",
    "        \"canonical_path\": canonical,\n",
    "        \"duplicate_count\": len(dupes),\n",
    "        \"duplicate_paths\": \";\".join(dupes),\n",
    "        \"all_paths\": \";\".join(paths_sorted),\n",
    "        \"splits_involved\": \",\".join(splits),\n",
    "        \"cross_split\": int(len(splits) > 1),\n",
    "    }\n",
    "\n",
    "def print_groups(title: str, groups: list, max_groups: int = 50):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[REPORT] {title}\")\n",
    "    print(\"=\" * 90)\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "        return\n",
    "\n",
    "    shown = 0\n",
    "    for i, g in enumerate(groups, start=1):\n",
    "        print(f\"\\n[{i}] Canonical: {g['canonical_path']}\")\n",
    "        print(f\"    Duplicates found: {g['duplicate_count']}\")\n",
    "        print(f\"    Splits involved: {g['splits_involved']} (cross_split={bool(g['cross_split'])})\")\n",
    "        if g[\"duplicate_count\"] > 0:\n",
    "            for dp in g[\"duplicate_paths\"].split(\";\"):\n",
    "                if dp.strip():\n",
    "                    print(f\"      - {dp}\")\n",
    "        shown += 1\n",
    "        if shown >= max_groups:\n",
    "            remaining = len(groups) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"\\n... (showing first {max_groups} groups, {remaining} more not shown)\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Main scanner\n",
    "# -----------------------------\n",
    "def run_duplicate_scan(\n",
    "    dataset_root: str,\n",
    "    report_dir: str = None,\n",
    "    do_visual: bool = True,\n",
    "    phash_bits: int = 16,\n",
    "    print_max_groups: int = 50,\n",
    "    ignore_dirs=(\"dup_reports\", \".git\", \"__pycache__\"),\n",
    "):\n",
    "    dataset_root = Path(dataset_root)\n",
    "    assert dataset_root.exists(), f\"Path does not exist: {dataset_root}\"\n",
    "\n",
    "    report_dir = Path(report_dir) if report_dir else (dataset_root / \"dup_reports\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(iter_image_files(dataset_root, ignore_dirs=set(ignore_dirs)))\n",
    "    print(f\"[INFO] Found {len(files):,} image files under: {dataset_root}\")\n",
    "\n",
    "    bad_files = []\n",
    "    exact_map = {}   # sha256 -> [paths]\n",
    "    visual_map = {}  # phash  -> [paths]\n",
    "\n",
    "    # ---- EXACT hashing pass ----\n",
    "    for p in tqdm(files, desc=\"Hashing (SHA-256)\", unit=\"img\"):\n",
    "        try:\n",
    "            h = sha256_file(p)\n",
    "            exact_map.setdefault(h, []).append(p)\n",
    "        except Exception as e:\n",
    "            bad_files.append((str(p), f\"sha256_error: {e}\"))\n",
    "\n",
    "    # ---- VISUAL hashing pass ----\n",
    "    if do_visual:\n",
    "        for p in tqdm(files, desc=f\"Hashing (pHash-{phash_bits})\", unit=\"img\"):\n",
    "            try:\n",
    "                h = phash_image(p, hash_size=phash_bits)\n",
    "                visual_map.setdefault(h, []).append(p)\n",
    "            except Exception as e:\n",
    "                bad_files.append((str(p), f\"phash_error: {e}\"))\n",
    "\n",
    "    # ---- Build groups ----\n",
    "    exact_groups = []\n",
    "    for k, ps in exact_map.items():\n",
    "        if len(ps) >= 2:\n",
    "            exact_groups.append(_group_to_row(\"exact_sha256\", k, ps, dataset_root))\n",
    "\n",
    "    visual_groups = []\n",
    "    if do_visual:\n",
    "        for k, ps in visual_map.items():\n",
    "            if len(ps) >= 2:\n",
    "                visual_groups.append(_group_to_row(\"visual_phash\", k, ps, dataset_root))\n",
    "\n",
    "    exact_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "    visual_groups.sort(key=lambda d: (-d[\"duplicate_count\"], d[\"canonical_path\"]))\n",
    "\n",
    "    # ---- Save CSVs (Option A: ALWAYS include headers) ----\n",
    "    cols = [\n",
    "        \"group_type\",\n",
    "        \"group_key\",\n",
    "        \"canonical_path\",\n",
    "        \"duplicate_count\",\n",
    "        \"duplicate_paths\",\n",
    "        \"all_paths\",\n",
    "        \"splits_involved\",\n",
    "        \"cross_split\",\n",
    "    ]\n",
    "\n",
    "    exact_csv = report_dir / \"duplicates_exact_sha256.csv\"\n",
    "    visual_csv = report_dir / \"duplicates_visual_phash.csv\"\n",
    "    combined_csv = report_dir / \"duplicates_combined.csv\"\n",
    "    bad_csv = report_dir / \"bad_or_unreadable.csv\"\n",
    "    summary_json = report_dir / \"summary.json\"\n",
    "\n",
    "    pd.DataFrame(exact_groups, columns=cols).to_csv(exact_csv, index=False)\n",
    "\n",
    "    if do_visual:\n",
    "        pd.DataFrame(visual_groups, columns=cols).to_csv(visual_csv, index=False)\n",
    "        pd.DataFrame(exact_groups + visual_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(exact_groups, columns=cols).to_csv(combined_csv, index=False)\n",
    "\n",
    "    pd.DataFrame(bad_files, columns=[\"path\", \"error\"]).to_csv(bad_csv, index=False)\n",
    "\n",
    "    # ---- Print reports ----\n",
    "    print_groups(\"EXACT duplicates (same bytes / SHA-256)\", exact_groups, max_groups=print_max_groups)\n",
    "    if do_visual:\n",
    "        print_groups(f\"VISUAL duplicates (pHash hash_size={phash_bits})\", visual_groups, max_groups=print_max_groups)\n",
    "\n",
    "    # ---- Summary ----\n",
    "    exact_dup_groups = len(exact_groups)\n",
    "    visual_dup_groups = len(visual_groups) if do_visual else 0\n",
    "    exact_dup_images = sum(g[\"duplicate_count\"] for g in exact_groups)\n",
    "    visual_dup_images = sum(g[\"duplicate_count\"] for g in visual_groups) if do_visual else 0\n",
    "    exact_cross = sum(g[\"cross_split\"] for g in exact_groups)\n",
    "    visual_cross = sum(g[\"cross_split\"] for g in visual_groups) if do_visual else 0\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_root\": str(dataset_root),\n",
    "        \"total_images_scanned\": len(files),\n",
    "        \"bad_or_unreadable_images\": len(bad_files),\n",
    "        \"exact_duplicate_groups\": exact_dup_groups,\n",
    "        \"exact_duplicate_images_excluding_canonicals\": int(exact_dup_images),\n",
    "        \"exact_groups_cross_split\": int(exact_cross),\n",
    "        \"visual_duplicate_groups\": int(visual_dup_groups),\n",
    "        \"visual_duplicate_images_excluding_canonicals\": int(visual_dup_images),\n",
    "        \"visual_groups_cross_split\": int(visual_cross),\n",
    "        \"reports_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"[SUMMARY]\")\n",
    "    print(f\"Total images scanned: {len(files):,}\")\n",
    "    print(f\"Bad/unreadable images: {len(bad_files):,}\")\n",
    "    print(f\"Exact duplicate groups: {exact_dup_groups:,} (cross-split groups: {exact_cross:,})\")\n",
    "    if do_visual:\n",
    "        print(f\"Visual duplicate groups: {visual_dup_groups:,} (cross-split groups: {visual_cross:,})\")\n",
    "    print(f\"Saved reports to: {report_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"exact_groups\": exact_groups,\n",
    "        \"visual_groups\": visual_groups,\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"exact_csv\": str(exact_csv),\n",
    "        \"visual_csv\": str(visual_csv) if do_visual else None,\n",
    "        \"combined_csv\": str(combined_csv),\n",
    "        \"bad_csv\": str(bad_csv),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# RUN using the config above\n",
    "# -----------------------------\n",
    "result = run_duplicate_scan(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    report_dir=REPORT_DIR,\n",
    "    do_visual=DO_VISUAL,\n",
    "    phash_bits=PHASH_BITS,\n",
    "    print_max_groups=PRINT_MAX_GROUPS,\n",
    "    ignore_dirs=IGNORE_DIRS,\n",
    ")\n",
    "\n",
    "# This will no longer crash even when no duplicates exist (CSV still has headers)\n",
    "print(\"\\n[INFO] Preview:\", result[\"combined_csv\"])\n",
    "print(pd.read_csv(result[\"combined_csv\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bea54-1e88-4d54-a917-345fb22c2646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
