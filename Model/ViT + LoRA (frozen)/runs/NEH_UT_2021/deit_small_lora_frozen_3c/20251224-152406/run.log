
=== DATASET RUN START: NEH_UT_2021 ===

[DATA] Split/Class counts:
class   CNV  DRUSEN  NORMAL  TOTAL
split                             
test    544     687    1292   2523
train  2188    3525    5791  11504
val     471     690    1475   2636
[DATA] Train imbalance: min=2188, max=5791, ratio=2.65
[RAM] Caching split='train' images=11504 as uint8 tensors. Estimated RAM ~ 1.61 GB
[RAM] Cached 11504 images for split='train'
[RAM] Caching split='val' images=2636 as uint8 tensors. Estimated RAM ~ 0.37 GB
[RAM] Cached 2636 images for split='val'
[RAM] Caching split='test' images=2523 as uint8 tensors. Estimated RAM ~ 0.35 GB
[RAM] Cached 2523 images for split='test'
[IMB] Train class counts: [2188, 3525, 5791]
[IMB] Class-Balanced weights (mean~1): [1.4228, 0.9412, 0.636]
[IMB] Sampler disabled. min=2188 max=5791 ratio=2.65

[DATASET] NEH_UT_2021
 - root: D:\AIUB\DSP\Code\Datasets\NEH_UT_2021RetinalOCTDataset\NEH_UT_2021RetinalOCTDataset_V2_CLEAN_SHAONLY
 - pad_to_square: True
 - cache_in_ram: True
 - classes (3): ['CNV', 'DRUSEN', 'NORMAL']
 - split sizes: train=11504 val=2636 test=2523

[MODEL] Creating deit_small_distilled_patch16_224 + Manual LoRA (frozen=True)
[PARAMS] total=21,816,198 trainable=149,766 (0.6865%)
[IMB] Train class counts: [2188, 3525, 5791]
[IMB] Class-Balanced weights (mean~1): [1.4228, 0.9412, 0.636]
[IMB] Sampler disabled. min=2188 max=5791 ratio=2.65
[NEH_UT_2021] Epoch 001/100 | lr=1.00e-04 | train_loss=0.9110 train_acc=60.83% | val_loss=0.7176 val_acc=81.03% val_macroF1=78.26% | ep_time=16.5s peakVRAM=1.75GB
[ES] New best val_loss=0.717642 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 002/100 | lr=1.50e-04 | train_loss=0.7262 train_acc=74.91% | val_loss=0.6550 val_acc=84.90% val_macroF1=82.71% | ep_time=16.4s peakVRAM=1.75GB
[ES] New best val_loss=0.655003 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 003/100 | lr=2.00e-04 | train_loss=0.6786 train_acc=78.26% | val_loss=0.6242 val_acc=85.96% val_macroF1=84.23% | ep_time=16.4s peakVRAM=1.75GB
[ES] New best val_loss=0.624178 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 004/100 | lr=2.50e-04 | train_loss=0.6523 train_acc=80.00% | val_loss=0.6400 val_acc=86.46% val_macroF1=84.91% | ep_time=16.5s peakVRAM=1.75GB
[ES] No improve 1/10
[NEH_UT_2021] Epoch 005/100 | lr=3.00e-04 | train_loss=0.6346 train_acc=81.50% | val_loss=0.6099 val_acc=87.29% val_macroF1=85.80% | ep_time=16.3s peakVRAM=1.75GB
[ES] New best val_loss=0.609864 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 006/100 | lr=3.50e-04 | train_loss=0.6119 train_acc=82.99% | val_loss=0.5988 val_acc=87.67% val_macroF1=86.56% | ep_time=16.2s peakVRAM=1.75GB
[ES] New best val_loss=0.598766 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 007/100 | lr=4.00e-04 | train_loss=0.6047 train_acc=83.45% | val_loss=0.5974 val_acc=87.33% val_macroF1=86.71% | ep_time=16.2s peakVRAM=1.75GB
[ES] New best val_loss=0.597372 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 008/100 | lr=4.50e-04 | train_loss=0.6026 train_acc=83.32% | val_loss=0.5934 val_acc=88.05% val_macroF1=87.13% | ep_time=16.3s peakVRAM=1.75GB
[ES] New best val_loss=0.593351 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 009/100 | lr=5.00e-04 | train_loss=0.6020 train_acc=83.37% | val_loss=0.6565 val_acc=82.70% val_macroF1=83.64% | ep_time=16.3s peakVRAM=1.75GB
[ES] No improve 1/10
[NEH_UT_2021] Epoch 010/100 | lr=5.00e-04 | train_loss=0.6051 train_acc=83.01% | val_loss=0.5948 val_acc=87.75% val_macroF1=87.49% | ep_time=16.4s peakVRAM=1.75GB
[ES] No improve 2/10
[NEH_UT_2021] Epoch 011/100 | lr=5.00e-04 | train_loss=0.5966 train_acc=83.51% | val_loss=0.5903 val_acc=87.75% val_macroF1=86.72% | ep_time=16.4s peakVRAM=1.75GB
[ES] New best val_loss=0.590287 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 012/100 | lr=4.99e-04 | train_loss=0.5838 train_acc=84.28% | val_loss=0.5826 val_acc=88.62% val_macroF1=87.85% | ep_time=16.4s peakVRAM=1.75GB
[ES] New best val_loss=0.582648 -> saved runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406\best_model.pth
[NEH_UT_2021] Epoch 013/100 | lr=4.99e-04 | train_loss=0.5771 train_acc=85.16% | val_loss=0.6305 val_acc=84.71% val_macroF1=85.14% | ep_time=16.4s peakVRAM=1.75GB
[ES] No improve 1/10
[NEH_UT_2021] Epoch 014/100 | lr=4.98e-04 | train_loss=0.5704 train_acc=85.48% | val_loss=0.5895 val_acc=87.75% val_macroF1=87.02% | ep_time=16.3s peakVRAM=1.75GB
[ES] No improve 2/10
[NEH_UT_2021] Epoch 015/100 | lr=4.96e-04 | train_loss=0.5689 train_acc=85.31% | val_loss=0.5912 val_acc=88.20% val_macroF1=87.28% | ep_time=16.3s peakVRAM=1.75GB
[ES] No improve 3/10
[NEH_UT_2021] Epoch 016/100 | lr=4.95e-04 | train_loss=0.5680 train_acc=85.32% | val_loss=0.6194 val_acc=86.72% val_macroF1=85.18% | ep_time=16.3s peakVRAM=1.75GB
[ES] No improve 4/10
[NEH_UT_2021] Epoch 017/100 | lr=4.93e-04 | train_loss=0.5632 train_acc=86.20% | val_loss=0.5874 val_acc=88.05% val_macroF1=87.55% | ep_time=16.2s peakVRAM=1.75GB
[ES] No improve 5/10
[NEH_UT_2021] Epoch 018/100 | lr=4.90e-04 | train_loss=0.5643 train_acc=85.50% | val_loss=0.6159 val_acc=86.23% val_macroF1=86.27% | ep_time=16.2s peakVRAM=1.75GB
[ES] No improve 6/10
[NEH_UT_2021] Epoch 019/100 | lr=4.88e-04 | train_loss=0.5473 train_acc=86.56% | val_loss=0.6149 val_acc=86.31% val_macroF1=86.36% | ep_time=16.3s peakVRAM=1.75GB
[ES] No improve 7/10
[NEH_UT_2021] Epoch 020/100 | lr=4.85e-04 | train_loss=0.5473 train_acc=86.65% | val_loss=0.5947 val_acc=87.52% val_macroF1=87.01% | ep_time=16.2s peakVRAM=1.75GB
[ES] No improve 8/10
[NEH_UT_2021] Epoch 021/100 | lr=4.82e-04 | train_loss=0.5475 train_acc=86.90% | val_loss=0.6083 val_acc=86.99% val_macroF1=85.95% | ep_time=16.3s peakVRAM=1.75GB
[ES] No improve 9/10
[NEH_UT_2021] Epoch 022/100 | lr=4.78e-04 | train_loss=0.5278 train_acc=87.87% | val_loss=0.5905 val_acc=88.43% val_macroF1=87.65% | ep_time=16.2s peakVRAM=1.75GB
[ES] No improve 10/10
[NEH_UT_2021] Early stopping triggered.
[CAL] Fitting temperature scaling on VAL set...
[CAL] Learned temperature T = 0.6801

------------------------------------------------------------------------------------------
[NEH_UT_2021] TEST classification report (uncalibrated)
------------------------------------------------------------------------------------------
              precision    recall  f1-score   support

         CNV     0.8646    0.9393    0.9004       544
      DRUSEN     0.8466    0.6987    0.7656       687
      NORMAL     0.8952    0.9458    0.9198      1292

    accuracy                         0.8771      2523
   macro avg     0.8688    0.8613    0.8619      2523
weighted avg     0.8754    0.8771    0.8736      2523


==========================================================================================
[NEH_UT_2021] TEST summary (uncalibrated): Acc=87.71% MacroF1=86.19% ECE=0.0715 NLL=0.3757 Brier=0.1955
[NEH_UT_2021] Macro-AUC=0.9565
[NEH_UT_2021] TEMP-SCALED (T=0.680): ECE=0.0151 NLL=0.3430 Brier=0.1866
[NEH_UT_2021] Params trainable: 149,766 (0.6865%)
[NEH_UT_2021] Adapter-only size: 0.58 MB
[NEH_UT_2021] Inference latency (B=1): 13.639 ms/img (73.3 imgs/s), peakVRAM=0.10GB
[NEH_UT_2021] Inference throughput (B=64): 2530.1 imgs/s, peakVRAM=0.29GB
[NEH_UT_2021] Outputs saved in: runs\NEH_UT_2021\deit_small_lora_frozen_3c\20251224-152406
==========================================================================================

